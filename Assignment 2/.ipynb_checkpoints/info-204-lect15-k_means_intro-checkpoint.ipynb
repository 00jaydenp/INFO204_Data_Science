{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e51706-a026-47e4-ac69-6b6e4397b741",
   "metadata": {},
   "source": [
    "# $k$-Means Clustering\n",
    "\n",
    "The $k$-Means clustering algorithm is a very simple approach to clustering that attempts to identify $k$ subgroups of instances in a data set through an iterative process. The example below uses $k$-Means to select subgroups for a simple two-dimensional data set: although the data set is simple, it is sufficient for demonstrating the basic properties of $k$-Means, and the process of applying the algorithm to more complex data (e.g., higher dimensionality or more instances) is actually no different from the process shown here!\n",
    "\n",
    "We start by importing our required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a9a76-fdcc-4ce5-bbcc-bd5f2fdedf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b180865-b1c2-443d-a638-5ced17dfcc4a",
   "metadata": {},
   "source": [
    "Next, we need some (unlabelled) data. Here, we use the [make_blobs()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) function (which usually returns an assigned class for each instance, but in this case we will ignore it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf3704-ca90-4762-bc5a-1d0d92243708",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4000\n",
    "n_components = 4\n",
    "\n",
    "X, _ = make_blobs(n_samples=n_samples,\n",
    "                  centers=n_components,\n",
    "                  cluster_std=0.60,\n",
    "                  random_state=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f9614-666f-45e8-a10a-ac7c708a4080",
   "metadata": {},
   "source": [
    "Humans are quite good at picking out clusters (so good, in fact, that we often see clusters where none actually exist!), so it's pretty easy to see that there are four clusters in this data. However, the $k$-Means algorithm does not know this, so let's see it in action. We start by defining up-front a desired number of clusters to form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fcf15-f5ec-41c8-ac42-5d03522dcca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4 ## a rough guess! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d27779-473c-4c92-896d-c7fca3379eed",
   "metadata": {},
   "source": [
    "Then, we can define the $k$-Means model (in this case, we have used a pipeline, as it's a good idea to standardise the incoming data - why?) and fit it to our unlabelled data. Notice how in this instance, the call to `fit()` does not include a corresponding `y` parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb9d23-2337-4a0a-bd44-cde5e99f17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('cluster', KMeans(random_state=1234, n_clusters=num_clusters, init='k-means++'))\n",
    "]).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0cf00-070a-43a8-ac54-c537a0c7bc1d",
   "metadata": {},
   "source": [
    "Now we can extract the details of our fit. We are interested in two attributes:\n",
    "  1. `labels_`: which returns us the cluster that each instance was assiged to by the algorithm\n",
    "  2. `cluster_centers_`: which returns us the array of cluster centres (i.e., the centroid of where the cluster is located in space)\n",
    "  \n",
    "Notice that, because we standardised our data, the cluster centres are also standardised - if we want to plot them against our original data then we need to inverse transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e92fa-5358-4652-b1ac-dcd4a26c122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = cluster['cluster'].labels_\n",
    "zcentroids = cluster['cluster'].cluster_centers_\n",
    "centroids = cluster['scale'].inverse_transform(zcentroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac3dc3-14f5-43b7-b35f-6e0911f003e8",
   "metadata": {},
   "source": [
    "Now all that is left to do is interrogate the result - in this case, we will plot the data again, but this time colour the data according to the cluster into which it was assigned. We will also plot the cluster centres for good measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1529674-6b81-4cef-83b6-e727e2b8297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, palette='Set1');\n",
    "sns.scatterplot(x=centroids[:, 0], y=centroids[:, 1], color='black', s=200);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a6a7b-4e57-4cc7-bee6-3a653d0e462b",
   "metadata": {},
   "source": [
    "Perhaps unsurprisingly, given that we knew how many clusters we needed, the algorithm has done a good job identifying the required clusters and assigning instances to the clusters. But what happens if we get the number of clusters \"wrong\"?\n",
    "\n",
    "## Specifying $k$-Means Clusters\n",
    "Before estimating the \"right\" number of clusters, let's just explore what happens when the number of clusters provided to the algorithm differs from the \"ideal\" number of clusters actually in the data. We can do this with a simple loop, given that the basics of the algorithm don't change (we're just supplying a new hyperparameter value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180140ca-a9e3-4a6e-bdbc-7598b106ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('cluster', KMeans(random_state=1234, n_clusters=1, init='k-means++'))\n",
    "])\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "for i, num_clusters in enumerate(range(1, 13)):\n",
    "    cluster.set_params(**{ 'cluster__n_clusters' : num_clusters })\n",
    "    cluster.fit(X)\n",
    "    \n",
    "    hue_labels = [ f'Group {i:02d}' for i in range(1, num_clusters + 1) ]\n",
    "    labels = [ f'Group {i + 1:02d}' for i in cluster['cluster'].labels_ ]\n",
    "    zcentroids = cluster['cluster'].cluster_centers_\n",
    "    centroids = cluster['scale'].inverse_transform(zcentroids)\n",
    "\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=labels, hue_order=hue_labels, palette='Set1', ax=axs[i // 4][i % 4])\n",
    "    sns.scatterplot(x=centroids[:, 0], y=centroids[:, 1], color='black', s=200, ax=axs[i // 4][i % 4])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8c3f7-9067-4359-8dc8-3cee59ff43f6",
   "metadata": {},
   "source": [
    "As can be seen, $k$-Means has no way of informing the user that the number of clusters is over or underspecified - if it is supplied $k$ clusters, it will use them all!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38aa18",
   "metadata": {},
   "source": [
    "## Estimating required number of clusters $k$ with Silhouette Analysis\n",
    "\n",
    "Picking the number of required clusters isn't easy (if you can work it out by looking at the data, you probably don't need to perform clustering!). However, as discussed in the lecture, we can use Silhouette Analysis to help guide us in this process - it's not perfect, but can get us into the right ballpark, at least.\n",
    "\n",
    "The following example shows how Silhouette Analysis is done in scikit-learn, and also demonstrates why weighted sum of squares (WSS) on its own is not a good measure for establishing the required number of clusters.\n",
    "\n",
    "Essentially, all we are doing here is trying k-Means clustering for a range of different $k$ values, and after each attempt to cluster, we are calling the `silhouette_score` function to compute $\\widetilde{s}(k)$. For comparison, we also record the WSS (scikit-learn calls this `inertia_`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690053e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('cluster', KMeans(random_state=1234, n_clusters=1, init='k-means++'))\n",
    "])\n",
    "\n",
    "scores = []\n",
    "for i, num_clusters in enumerate(range(2, 13)):\n",
    "    cluster.set_params(**{ 'cluster__n_clusters' : num_clusters })\n",
    "    C = cluster.fit_predict(X) ## note the use of fit_predict(X) rather than cluster.fit(X), and then C = cluster.predict(X)\n",
    "    \n",
    "    scores.append({ 'k' : num_clusters, 's(k)' : silhouette_score(X, C), 'WSS(k)' : cluster['cluster'].inertia_ })\n",
    "scores = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766cad12",
   "metadata": {},
   "source": [
    "With all that in place, let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = scores[scores['s(k)']==scores['s(k)'].max()]\n",
    "\n",
    "with sns.plotting_context(rc={ 'axes.labelsize' : 24, 'xtick.labelsize' : 16, 'ytick.labelsize' : 16 }):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    sns.lineplot(data=scores, x='k', y='WSS(k)', ax=axs[0]).set(xlabel='$k$', ylabel='$WSS(k)$')\n",
    "    axs[0].scatter(best_k[['k']], best_k[['WSS(k)']], s=100, facecolors='none', edgecolors='#ce2227')\n",
    "    sns.lineplot(data=scores, x='k', y='s(k)', ax=axs[1]).set(xlabel='$k$', ylabel='$\\widetilde{s}(k)$')\n",
    "    axs[1].scatter(best_k[['k']], best_k[['s(k)']], s=100, facecolors='none', edgecolors='#ce2227')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4be266",
   "metadata": {},
   "source": [
    "As can be seen, $k=4$ is a clear elbow point in the WSS values, and the same value has been picked by Silhouette Analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('info204')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8cee10d90c8bacaf9909a99546f7b02b34bbd8407eebea03a6d2ead7eeabc0dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
